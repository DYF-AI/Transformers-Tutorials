{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2995c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import datasets\n",
    "from datasets import load_dataset,load_from_disk\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from modeling_layoutxlm_re import LayoutLMv2ForRelationExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3834ab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_visual_backbone: False\n"
     ]
    }
   ],
   "source": [
    "DP = \"/mnt/j/dataset/document-intelligence/XFUND/fr\"\n",
    "OP = \"/mnt/j/dataset/document-intelligence/XFUND/output\"\n",
    "MP = \"/mnt/j/model/pretrained-model/bert_torch\"\n",
    "TRAINED_MP = \"./output/layoutxlm-finetuned-xfund-re/\"\n",
    "\n",
    "use_visual_backbone=False\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(os.path.join(MP, \"microsoft-layoutxlm-base\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained(os.path.join(TRAINED_MP, \"checkpoint-5000\"))\n",
    "#model = LayoutLMv2ForRelationExtraction.from_pretrained(os.path.join(MP, \"microsoft-layoutxlm-base\"),use_visual_backbone=use_visual_backbone)\n",
    "# re训练的模型\n",
    "model = LayoutLMv2ForRelationExtraction.from_pretrained(os.path.join(TRAINED_MP, \"checkpoint-5000\"),use_visual_backbone=use_visual_backbone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59dc2ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'input_ids', 'bbox', 'labels', 'image', 'original_image', 'entities', 'relations'],\n",
      "        num_rows: 202\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'input_ids', 'bbox', 'labels', 'image', 'original_image', 'entities', 'relations'],\n",
      "        num_rows: 71\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = Dataset.from_file(os.path.join(DP, \"fr.train.arrow\"))\n",
    "test_dataset = Dataset.from_file(os.path.join(DP, \"fr.val.arrow\"))\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": test_dataset})\n",
    "print(\"dataset:\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "409c1b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'input_ids', 'bbox', 'labels', 'image', 'original_image', 'entities', 'relations'])\n",
      "example['input_ids']: [72791, 2149, 104, 25, 101287, 990, 106, 22364, 61038, 35213, 1514, 335, 76783, 11338, 109893, 122112, 84780, 31244, 51552, 441, 132824, 35213, 14614, 167795, 618, 89097, 85621, 7, 62, 5, 61038, 35213, 238530, 40713, 839, 80020, 615, 55448, 628, 38016, 149896, 13, 8, 21, 17752, 22, 6, 102250, 115, 19183, 94228, 531, 104067, 12, 438, 39, 6470, 68915, 170158, 6, 58745, 14154, 60221, 166132, 4, 436, 187367, 17166, 7, 628, 6, 138971, 64, 54601, 1830, 628, 123708, 64, 166697, 13, 628, 5054, 9942, 384, 50763, 26551, 132, 7, 16, 6, 58745, 4880, 192508, 196417, 3768, 72813, 25388, 148573, 15, 112, 96, 25, 165264, 16, 6, 191712, 981, 84624, 5, 277, 20897, 64, 88154, 64, 22469, 38016, 48218, 1426, 5, 191712, 5, 277, 787, 2041, 2312, 80273, 13, 104, 25, 40098, 12, 15, 16305, 27227, 104, 25, 309, 1193, 21, 108090, 42518, 1236, 253, 2771, 49894, 16, 6, 166697, 13, 20, 6, 179149, 8, 5054, 6, 136711, 55448, 11129, 628, 60013, 916, 44386, 6, 136711, 14400, 93781, 253, 1284, 123107, 3190, 6, 136711, 56187, 83221, 8, 45556, 7, 20, 127847, 7, 8, 5054, 6, 136711, 113800, 40567, 628, 29096, 6, 136711, 113800, 29208, 6, 136711, 6, 166697, 11, 76659, 16867, 26988, 6, 136711, 52236, 107, 108090, 15, 7, 11870, 238, 85789, 16, 6, 136711, 1031, 853, 211161, 1236, 179621, 8, 96, 25, 40098, 12, 438, 39, 167599, 71034, 10763, 36166, 58503, 44386, 384, 50763, 26551, 4880, 169842, 122381, 1530, 72813, 25388, 452, 6827, 981, 84624, 5, 277, 1398, 15270, 86, 28022, 115, 19183, 94228, 531, 104067, 12, 438, 39, 24479, 171799, 2312, 628, 32348, 803, 4273, 238, 19077, 384, 50763, 26551, 4880, 19051, 304, 151616, 3117, 72813, 25388, 145010, 647, 981, 84624, 5, 277]\n",
      "example['input_ids'] len: 283\n"
     ]
    }
   ],
   "source": [
    "example = dataset['train'][0]\n",
    "print(example.keys())\n",
    "# print(example)\n",
    "print(\"example['input_ids']:\",example['input_ids'])\n",
    "print(\"example['input_ids'] len:\",len(example['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debd8155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.csdn.net/gjj19901005/article/details/127530571\n",
    "\n",
    "# dummy_model_input = tokenizer([\"This is a sample\"], return_tensors=None)\n",
    " \n",
    "# dummy_model_input['input_ids'] = torch.zeros((1,512),dtype=torch.int64) + 10\n",
    "# dummy_model_input['bbox'] = torch.zeros((1,512, 4), dtype=torch.int64) + 20\n",
    "# dummy_model_input['attention_mask'] = torch.zeros((1,512), dtype=torch.int64) + 1\n",
    "# dummy_model_input['start'] = torch.tensor(np.asarray([[0,3,5,7,9,-1]], dtype=np.int64).reshape((1, -1)))\n",
    "# # dummy_model_input['end'] = torch.tensor(np.asarray([3,5,7,9,15], dtype=np.int64).reshape((1, -1)))\n",
    "# dummy_model_input['labels'] = torch.tensor(np.asarray([[1, 2, 1, 2, 1]], dtype=np.int64).reshape((1, -1)))\n",
    "# dummy_model_input['head'] = torch.tensor(np.asarray([[0,2,4]], dtype=np.int64).reshape((1, -1)))\n",
    "# dummy_model_input['tail'] = torch.tensor(np.asarray([[1,3,3]], dtype=np.int64).reshape((1, -1)))\n",
    "\n",
    "\n",
    "# print(dummy_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d1d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# torch.onnx.export(\n",
    "#     model, \n",
    "#     (dummy_model_input['input_ids'], \n",
    "#     dummy_model_input['bbox'], \n",
    "#     dummy_model_input['attention_mask'], \n",
    "#     dummy_model_input['start'],\n",
    "#     # dummy_model_input['end'],\n",
    "#     dummy_model_input['labels'],\n",
    "#     dummy_model_input['head'],\n",
    "#     dummy_model_input['tail']),\n",
    "#     f=\"./layoutxlm_re.onnx\",  \n",
    "#     input_names=['input_ids','bbox','attention_mask', 'start', 'labels', 'head', 'tail'], \n",
    "#     output_names=['logits'], \n",
    "#     dynamic_axes={'input_ids': {0: 'batch_size'}, \n",
    "#                   'bbox': {0: 'batch_size'},\n",
    "#                   'attention_mask': {0: 'batch_size'}, \n",
    "#                   'start': {0: 'batch_size', 1: 'entity_len'}, \n",
    "#                 #   'end': {0: 'batch_size', 1: 'entity_len'},\n",
    "#                   'labels': {0: 'batch_size', 1: 'entity_len'}, \n",
    "#                   'head': {0: 'batch_size', 1: 'relation_len'},\n",
    "#                   'tail': {0: 'batch_size', 1: 'relation_len'}, \n",
    "#                   'logits':{0: 'batch_size', 1:'relation_len'}\n",
    "#                   }, \n",
    "#     # do_constant_folding=True, \n",
    "#     # verbose=True,\n",
    "#     opset_version=13, \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59154f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/j/dataset/document-intelligence/XFUND/fr/fr.val/fr_val_0.jpg\n",
      "item: {'input_ids': [12490, 2609, 68819, 169, 741, 91870, 5849, 199, 110267, 7, 41, 1023, 81418, 163465, 578, 233404, 4178, 48522, 7, 15, 57595, 7, 4, 60525, 4, 149592, 4, 3021, 5, 194, 182946, 3190, 5, 12498, 16277, 19116, 9, 13048, 113575, 44918, 8, 96, 25, 284, 36737, 253, 51347, 152, 76106, 82, 118009, 8, 87, 25, 683, 6371, 32, 77035, 28636, 224, 5470, 7, 72054, 77035, 28636, 199, 28486, 7, 94505, 628, 104, 25, 19988, 831, 10442, 4, 115953, 42, 77035, 28636, 95, 333, 1606, 104, 25, 309, 1764, 4002, 15, 272, 68819, 169, 115953, 42, 16, 77035, 67362, 2921, 152, 572, 5, 125508, 16271, 84780, 845, 230998, 41, 199, 32565, 94228, 90, 807, 405, 123780, 82, 199, 60525, 1059, 9, 513, 4288, 7, 2045, 34913, 532, 19640, 82, 34913, 27430, 19736, 5, 821, 25, 28792, 3075, 95, 123054, 104, 25, 81317, 7, 253, 180150, 199, 32565, 94228, 90, 807, 95, 39397, 12937, 4, 113, 48402, 199, 32565, 141859, 7, 4, 82, 199, 138488, 82, 52193, 100231, 10840, 253, 180150, 5460, 90364, 253, 96, 25, 446, 42077, 8, 21, 29806, 71357, 13, 5, 821, 25, 28792, 3075, 14919, 96, 25, 670, 6371, 253, 76662, 199, 32565, 82, 199, 43079, 41, 1647, 25, 508, 84494, 90, 253, 224, 7675, 8, 82641, 78, 291, 89261, 437, 73487, 13388, 5, 845, 232182, 22, 1810, 107, 12036, 3480, 21, 155358, 8, 39452, 1363, 1609, 96, 25, 670, 6371, 82, 4, 78, 291, 89261, 437, 73487, 13388, 4, 55, 653, 25, 508, 68429, 129344, 253, 101703, 55230, 253, 775, 59926, 5, 845, 52268, 7, 14919, 41, 96, 25, 670, 6371, 4372, 3900, 32395, 56, 224, 34919, 4, 113, 48402, 224, 34919, 176288, 4, 253, 96, 25, 309, 8, 3401, 138488, 4, 10274, 2970, 7675, 8, 96, 25, 169703, 8, 21, 89261, 4, 10274, 4, 78, 31929, 9, 318, 437, 73487, 13388, 4, 2970, 7675, 231433, 1029, 8493, 807, 21, 155358, 8, 39452, 1363, 115, 6, 89072, 5, 845, 52268, 7, 22, 1810, 107, 1103, 25, 379, 653, 25, 53, 10, 50734, 188118, 171197, 7722, 13993, 21, 71746, 115, 123054, 104, 25, 81317, 7, 4, 113, 48402, 22, 4611, 8, 351, 9, 7, 188786, 628, 8, 456, 4249, 4, 628, 78, 773, 39452, 1363, 437, 61047, 4170, 732, 41, 95, 56578, 1236, 108, 137464, 452, 95, 1774, 8, 13833, 209760, 807, 199, 93040, 7, 82791, 7, 628, 41, 23814, 9, 1059, 437, 164276, 3739, 8, 21164, 61379, 24500, 7, 2733, 34418, 21, 49907, 178805, 2320, 8, 96, 25, 670, 6371, 5, 339, 25, 670, 6371, 10, 95, 29582, 8, 6, 34590, 720, 199, 5595, 54916, 628, 104, 25, 58218, 603, 5736, 154686, 104, 25, 19988, 127989, 628, 26359, 5, 59235, 152, 3775, 152, 438, 39, 152, 786, 37214, 99428, 152, 24479, 5547, 628, 531, 9901, 8, 31124, 26455, 68819, 169, 1747, 100587, 95, 9023, 8918, 64, 176834, 8, 21, 33590, 569, 45618, 21, 19676, 531, 9901, 104, 25, 1811, 49894, 16, 5906, 446, 25512, 82, 23414, 152, 26729, 141762, 152, 72791, 2149, 8, 164492, 201, 24479, 77035, 77035, 427, 44586, 32196, 366, 164], 'bbox': [[165, 112, 189, 127], [193, 112, 258, 128], [193, 112, 258, 128], [193, 112, 258, 128], [262, 112, 326, 128], [262, 112, 326, 128], [330, 112, 370, 128], [374, 112, 399, 128], [404, 112, 466, 129], [404, 112, 466, 129], [470, 112, 504, 129], [508, 113, 549, 129], [552, 113, 637, 129], [640, 114, 704, 130], [707, 115, 752, 130], [165, 129, 251, 145], [255, 129, 285, 145], [290, 129, 373, 145], [290, 129, 373, 145], [377, 129, 447, 145], [377, 129, 447, 145], [377, 129, 447, 145], [377, 129, 447, 145], [451, 129, 554, 145], [451, 129, 554, 145], [558, 129, 619, 146], [558, 129, 619, 146], [623, 130, 664, 146], [623, 130, 664, 146], [623, 130, 664, 146], [668, 130, 752, 147], [668, 130, 752, 147], [668, 130, 752, 147], [164, 188, 192, 202], [196, 188, 286, 203], [290, 188, 377, 204], [290, 188, 377, 204], [290, 188, 377, 204], [381, 188, 454, 205], [458, 188, 511, 206], [515, 188, 538, 206], [541, 188, 598, 206], [541, 188, 598, 206], [541, 188, 598, 206], [541, 188, 598, 206], [602, 189, 613, 206], [618, 189, 697, 206], [701, 189, 706, 206], [710, 189, 802, 207], [166, 205, 182, 218], [186, 204, 302, 220], [307, 206, 332, 217], [335, 204, 391, 217], [335, 204, 391, 217], [335, 204, 391, 217], [335, 204, 391, 217], [335, 204, 391, 217], [184, 229, 210, 246], [221, 230, 246, 243], [249, 231, 277, 244], [280, 231, 345, 244], [280, 231, 345, 244], [348, 231, 461, 244], [182, 253, 209, 270], [221, 256, 245, 268], [247, 256, 270, 268], [273, 256, 331, 268], [273, 256, 331, 268], [333, 256, 392, 269], [394, 256, 415, 269], [418, 256, 479, 269], [418, 256, 479, 269], [418, 256, 479, 269], [481, 256, 541, 270], [481, 256, 541, 270], [481, 256, 541, 270], [543, 256, 606, 270], [543, 256, 606, 270], [182, 278, 209, 295], [218, 279, 243, 293], [246, 279, 262, 293], [266, 279, 301, 293], [266, 279, 301, 293], [304, 279, 338, 293], [304, 279, 338, 293], [304, 279, 338, 293], [342, 279, 368, 294], [372, 280, 406, 294], [410, 280, 471, 294], [410, 280, 471, 294], [410, 280, 471, 294], [410, 280, 471, 294], [475, 280, 543, 295], [475, 280, 543, 295], [475, 280, 543, 295], [181, 303, 209, 321], [218, 305, 261, 317], [218, 305, 261, 317], [264, 306, 271, 317], [162, 376, 185, 390], [162, 376, 185, 390], [217, 375, 372, 391], [217, 375, 372, 391], [217, 375, 372, 391], [159, 421, 170, 435], [174, 421, 229, 435], [233, 421, 260, 435], [264, 421, 286, 435], [289, 421, 388, 435], [392, 421, 474, 435], [392, 421, 474, 435], [478, 422, 514, 435], [518, 422, 533, 435], [537, 422, 616, 436], [618, 423, 633, 436], [637, 423, 658, 437], [661, 423, 746, 438], [749, 424, 810, 438], [749, 424, 810, 438], [749, 424, 810, 438], [749, 424, 810, 438], [749, 424, 810, 438], [159, 436, 191, 448], [194, 435, 277, 448], [194, 435, 277, 448], [194, 435, 277, 448], [280, 435, 297, 449], [300, 435, 385, 449], [300, 435, 385, 449], [300, 435, 385, 449], [300, 435, 385, 449], [387, 435, 458, 449], [387, 435, 458, 449], [387, 435, 458, 449], [387, 435, 458, 449], [460, 436, 477, 449], [480, 436, 535, 449], [537, 436, 606, 450], [537, 436, 606, 450], [537, 436, 606, 450], [537, 436, 606, 450], [609, 436, 620, 450], [623, 437, 679, 450], [681, 437, 706, 450], [708, 437, 808, 450], [160, 448, 244, 462], [160, 448, 244, 462], [249, 449, 289, 462], [294, 449, 312, 462], [317, 449, 382, 463], [385, 450, 470, 463], [385, 450, 470, 463], [473, 450, 484, 463], [491, 450, 556, 463], [560, 450, 586, 464], [589, 450, 691, 464], [695, 451, 787, 464], [695, 451, 787, 464], [695, 451, 787, 464], [789, 451, 808, 464], [159, 462, 180, 475], [183, 462, 274, 476], [277, 463, 296, 476], [299, 463, 413, 476], [299, 463, 413, 476], [299, 463, 413, 476], [416, 463, 427, 477], [430, 463, 487, 477], [490, 463, 524, 477], [527, 463, 625, 477], [628, 464, 639, 478], [642, 464, 696, 478], [642, 464, 696, 478], [642, 464, 696, 478], [642, 464, 696, 478], [700, 464, 721, 478], [724, 464, 742, 478], [745, 464, 808, 478], [160, 476, 233, 490], [160, 476, 233, 490], [160, 476, 233, 490], [236, 476, 309, 490], [236, 476, 309, 490], [236, 476, 309, 490], [236, 476, 309, 490], [313, 476, 397, 490], [400, 476, 437, 490], [400, 476, 437, 490], [400, 476, 437, 490], [400, 476, 437, 490], [440, 476, 450, 490], [454, 476, 510, 490], [513, 476, 536, 490], [539, 477, 639, 491], [642, 477, 659, 491], [662, 478, 686, 492], [689, 478, 747, 492], [750, 478, 780, 492], [783, 479, 807, 492], [783, 479, 807, 492], [783, 479, 807, 492], [157, 490, 220, 503], [157, 490, 220, 503], [223, 490, 234, 503], [237, 490, 265, 503], [268, 490, 298, 503], [301, 490, 323, 503], [326, 490, 408, 504], [412, 490, 425, 504], [429, 490, 454, 504], [457, 490, 545, 504], [549, 491, 572, 504], [576, 491, 639, 505], [576, 491, 639, 505], [576, 491, 639, 505], [642, 491, 658, 505], [661, 491, 737, 505], [739, 492, 760, 505], [763, 492, 807, 505], [763, 492, 807, 505], [158, 503, 193, 516], [197, 503, 211, 516], [215, 503, 229, 516], [233, 503, 320, 516], [324, 503, 344, 517], [348, 503, 432, 517], [348, 503, 432, 517], [436, 504, 469, 517], [473, 504, 509, 517], [473, 504, 509, 517], [473, 504, 509, 517], [473, 504, 509, 517], [512, 504, 532, 518], [512, 504, 532, 518], [535, 504, 547, 518], [551, 504, 575, 518], [579, 504, 665, 519], [668, 505, 691, 519], [695, 505, 759, 519], [695, 505, 759, 519], [695, 505, 759, 519], [762, 506, 777, 519], [780, 506, 807, 519], [780, 506, 807, 519], [780, 506, 807, 519], [158, 517, 213, 530], [215, 517, 314, 530], [318, 517, 329, 530], [333, 517, 406, 531], [408, 517, 456, 531], [459, 517, 470, 531], [475, 518, 504, 531], [507, 518, 575, 531], [507, 518, 575, 531], [578, 518, 595, 532], [598, 518, 685, 532], [598, 518, 685, 532], [688, 519, 772, 533], [775, 520, 807, 533], [157, 530, 190, 543], [157, 530, 190, 543], [157, 530, 190, 543], [157, 530, 190, 543], [193, 530, 231, 544], [234, 530, 312, 544], [234, 530, 312, 544], [234, 530, 312, 544], [316, 531, 345, 544], [348, 531, 418, 545], [348, 531, 418, 545], [422, 531, 433, 545], [437, 531, 499, 545], [503, 531, 531, 545], [535, 532, 601, 545], [604, 532, 707, 545], [604, 532, 707, 545], [710, 532, 720, 545], [723, 532, 753, 546], [723, 532, 753, 546], [723, 532, 753, 546], [756, 532, 777, 546], [780, 532, 809, 546], [157, 544, 248, 557], [157, 544, 248, 557], [251, 544, 282, 557], [284, 544, 314, 557], [317, 544, 349, 558], [352, 544, 372, 558], [375, 544, 461, 558], [375, 544, 461, 558], [375, 544, 461, 558], [465, 545, 486, 558], [489, 545, 504, 558], [508, 545, 600, 559], [508, 545, 600, 559], [604, 545, 637, 559], [604, 545, 637, 559], [640, 545, 653, 559], [656, 545, 711, 559], [656, 545, 711, 559], [656, 545, 711, 559], [714, 546, 739, 559], [742, 546, 806, 560], [742, 546, 806, 560], [742, 546, 806, 560], [158, 558, 183, 571], [186, 558, 214, 571], [216, 558, 291, 571], [216, 558, 291, 571], [216, 558, 291, 571], [294, 558, 331, 571], [334, 558, 347, 571], [351, 558, 437, 572], [440, 558, 460, 572], [463, 558, 547, 572], [463, 558, 547, 572], [550, 559, 568, 572], [572, 559, 572, 559], [572, 559, 599, 572], [572, 559, 599, 572], [602, 559, 617, 573], [620, 559, 705, 573], [620, 559, 705, 573], [708, 560, 727, 574], [730, 560, 772, 574], [730, 560, 772, 574], [775, 561, 808, 574], [775, 561, 808, 574], [775, 561, 808, 574], [156, 573, 177, 585], [156, 573, 177, 585], [156, 573, 177, 585], [180, 573, 190, 585], [193, 572, 240, 585], [243, 572, 300, 585], [303, 572, 374, 585], [377, 572, 439, 586], [442, 572, 491, 586], [494, 572, 509, 586], [512, 572, 574, 586], [577, 573, 598, 586], [601, 573, 653, 587], [656, 573, 729, 587], [656, 573, 729, 587], [656, 573, 729, 587], [656, 573, 729, 587], [656, 573, 729, 587], [731, 574, 741, 587], [744, 574, 807, 588], [156, 586, 175, 599], [179, 586, 207, 599], [210, 586, 232, 599], [235, 586, 340, 599], [235, 586, 340, 599], [235, 586, 340, 599], [235, 586, 340, 599], [344, 586, 366, 599], [372, 586, 393, 599], [396, 586, 439, 599], [396, 586, 439, 599], [396, 586, 439, 599], [442, 586, 463, 599], [468, 586, 483, 599], [485, 586, 518, 600], [522, 586, 606, 600], [522, 586, 606, 600], [610, 587, 636, 600], [639, 587, 710, 600], [639, 587, 710, 600], [714, 587, 753, 600], [756, 587, 787, 600], [791, 587, 806, 600], [156, 600, 218, 613], [156, 600, 218, 613], [221, 600, 240, 612], [243, 600, 314, 612], [317, 599, 345, 612], [348, 599, 361, 612], [364, 599, 398, 612], [402, 599, 421, 613], [425, 599, 471, 613], [475, 600, 531, 613], [533, 600, 569, 613], [572, 600, 593, 613], [596, 600, 641, 614], [596, 600, 641, 614], [643, 600, 707, 614], [643, 600, 707, 614], [709, 601, 730, 614], [732, 601, 761, 615], [764, 601, 806, 615], [764, 601, 806, 615], [156, 613, 167, 626], [170, 613, 192, 626], [195, 613, 270, 626], [273, 613, 330, 626], [333, 613, 353, 626], [356, 613, 408, 626], [412, 613, 501, 627], [412, 613, 501, 627], [412, 613, 501, 627], [412, 613, 501, 627], [504, 614, 544, 627], [547, 614, 561, 627], [564, 614, 604, 627], [607, 614, 701, 627], [607, 614, 701, 627], [704, 614, 722, 627], [726, 614, 764, 628], [726, 614, 764, 628], [726, 614, 764, 628], [726, 614, 764, 628], [726, 614, 764, 628], [767, 614, 806, 628], [767, 614, 806, 628], [767, 614, 806, 628], [767, 614, 806, 628], [155, 627, 163, 640], [166, 627, 181, 640], [183, 627, 221, 640], [224, 627, 243, 640], [246, 627, 246, 627], [246, 627, 297, 640], [246, 627, 297, 640], [300, 627, 324, 640], [327, 627, 421, 640], [327, 627, 421, 640], [425, 627, 445, 640], [448, 627, 520, 640], [448, 627, 520, 640], [448, 627, 520, 640], [448, 627, 520, 640], [523, 627, 558, 640], [561, 628, 613, 640], [617, 628, 677, 641], [617, 628, 677, 641], [617, 628, 677, 641], [681, 629, 736, 641], [740, 629, 760, 641], [763, 629, 798, 641], [763, 629, 798, 641], [154, 652, 181, 667], [184, 653, 193, 667], [327, 654, 361, 666], [364, 654, 374, 666], [154, 710, 185, 723], [154, 710, 185, 723], [194, 711, 203, 723], [318, 710, 397, 728], [318, 710, 397, 728], [156, 736, 216, 749], [220, 736, 229, 749], [328, 736, 390, 752], [155, 762, 190, 775], [193, 762, 213, 774], [217, 763, 237, 774], [241, 762, 272, 775], [280, 762, 302, 775], [153, 762, 315, 787], [153, 762, 315, 787], [153, 762, 315, 787], [153, 762, 315, 787], [216, 777, 275, 789], [216, 777, 275, 789], [277, 775, 290, 788], [294, 776, 343, 787], [294, 776, 343, 787], [294, 776, 343, 787], [153, 790, 209, 802], [214, 790, 232, 801], [234, 790, 250, 800], [252, 790, 303, 802], [306, 791, 330, 805], [152, 805, 215, 816], [219, 802, 231, 815], [235, 803, 303, 814], [306, 806, 327, 814], [329, 806, 363, 814], [154, 818, 191, 829], [154, 818, 191, 829], [154, 818, 191, 829], [195, 818, 292, 830], [195, 818, 292, 830], [450, 775, 515, 799], [450, 775, 515, 799], [149, 865, 187, 877], [190, 866, 206, 877], [209, 866, 239, 878], [242, 866, 253, 878], [543, 654, 616, 671], [543, 654, 616, 671], [620, 655, 631, 671], [659, 926, 735, 939], [659, 926, 735, 939], [738, 927, 756, 939], [759, 927, 847, 940], [860, 927, 871, 940], [648, 657, 716, 672], [199, 650, 224, 669], [381, 650, 406, 669], [324, 866, 431, 886], [324, 866, 431, 886], [324, 866, 431, 886], [500, 867, 554, 887], [500, 867, 554, 887]], 'labels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'B-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'B-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'B-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-QUESTION', 'I-QUESTION', 'B-QUESTION', 'I-QUESTION', 'B-QUESTION', 'I-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER', 'B-QUESTION', 'I-QUESTION', 'B-ANSWER', 'B-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'B-ANSWER', 'I-ANSWER', 'B-QUESTION', 'I-QUESTION', 'I-QUESTION', 'I-QUESTION', 'B-QUESTION', 'I-QUESTION', 'I-QUESTION', 'O', 'O', 'O', 'O', 'O', 'B-ANSWER', 'B-ANSWER', 'B-ANSWER', 'B-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER', 'I-ANSWER'], 'id': 'fr_val_0_0', 'image': array([[[255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255],\n",
      "        [255, 255, 255, ..., 255, 255, 255]]], dtype=uint8), 'original_image': <PIL.Image.Image image mode=RGB size=2480x3508 at 0x7FE39FC39D90>, 'entities': [{'start': 33, 'end': 57, 'label': 'QUESTION'}, {'start': 57, 'end': 63, 'label': 'ANSWER'}, {'start': 63, 'end': 78, 'label': 'ANSWER'}, {'start': 78, 'end': 95, 'label': 'ANSWER'}, {'start': 95, 'end': 99, 'label': 'ANSWER'}, {'start': 441, 'end': 443, 'label': 'QUESTION'}, {'start': 443, 'end': 445, 'label': 'QUESTION'}, {'start': 445, 'end': 448, 'label': 'QUESTION'}, {'start': 448, 'end': 450, 'label': 'ANSWER'}, {'start': 450, 'end': 452, 'label': 'QUESTION'}, {'start': 452, 'end': 453, 'label': 'ANSWER'}, {'start': 453, 'end': 483, 'label': 'QUESTION'}, {'start': 483, 'end': 485, 'label': 'ANSWER'}, {'start': 485, 'end': 489, 'label': 'QUESTION'}, {'start': 489, 'end': 492, 'label': 'QUESTION'}, {'start': 497, 'end': 498, 'label': 'ANSWER'}, {'start': 498, 'end': 499, 'label': 'ANSWER'}, {'start': 499, 'end': 500, 'label': 'ANSWER'}, {'start': 500, 'end': 505, 'label': 'ANSWER'}], 'relations': [{'head': 0, 'tail': 4, 'start_index': 33, 'end_index': 99}, {'head': 0, 'tail': 1, 'start_index': 33, 'end_index': 63}, {'head': 0, 'tail': 2, 'start_index': 33, 'end_index': 78}, {'head': 0, 'tail': 3, 'start_index': 33, 'end_index': 95}, {'head': 5, 'tail': 16, 'start_index': 441, 'end_index': 499}, {'head': 6, 'tail': 17, 'start_index': 443, 'end_index': 500}, {'head': 7, 'tail': 8, 'start_index': 445, 'end_index': 450}, {'head': 9, 'tail': 10, 'start_index': 450, 'end_index': 453}, {'head': 11, 'tail': 12, 'start_index': 453, 'end_index': 485}, {'head': 13, 'tail': 18, 'start_index': 485, 'end_index': 505}, {'head': 14, 'tail': 15, 'start_index': 489, 'end_index': 498}]}\n",
      "ccc: [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "label2id: {'HEADER': 0, 'QUESTION': 1, 'ANSWER': 2}\n",
      "entity_dict: {'start': [33, 57, 63, 78, 95, 441, 443, 445, 448, 450, 452, 453, 483, 485, 489, 497, 498, 499, 500], 'end': [57, 63, 78, 95, 99, 443, 445, 448, 450, 452, 453, 483, 485, 489, 492, 498, 499, 500, 505], 'label': [1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2]}\n",
      "model_input: {'input_ids': tensor([[ 12490,   2609,  68819,    169,    741,  91870,   5849,    199, 110267,\n",
      "              7,     41,   1023,  81418, 163465,    578, 233404,   4178,  48522,\n",
      "              7,     15,  57595,      7,      4,  60525,      4, 149592,      4,\n",
      "           3021,      5,    194, 182946,   3190,      5,  12498,  16277,  19116,\n",
      "              9,  13048, 113575,  44918,      8,     96,     25,    284,  36737,\n",
      "            253,  51347,    152,  76106,     82, 118009,      8,     87,     25,\n",
      "            683,   6371,     32,  77035,  28636,    224,   5470,      7,  72054,\n",
      "          77035,  28636,    199,  28486,      7,  94505,    628,    104,     25,\n",
      "          19988,    831,  10442,      4, 115953,     42,  77035,  28636,     95,\n",
      "            333,   1606,    104,     25,    309,   1764,   4002,     15,    272,\n",
      "          68819,    169, 115953,     42,     16,  77035,  67362,   2921,    152,\n",
      "            572,      5, 125508,  16271,  84780,    845, 230998,     41,    199,\n",
      "          32565,  94228,     90,    807,    405, 123780,     82,    199,  60525,\n",
      "           1059,      9,    513,   4288,      7,   2045,  34913,    532,  19640,\n",
      "             82,  34913,  27430,  19736,      5,    821,     25,  28792,   3075,\n",
      "             95, 123054,    104,     25,  81317,      7,    253, 180150,    199,\n",
      "          32565,  94228,     90,    807,     95,  39397,  12937,      4,    113,\n",
      "          48402,    199,  32565, 141859,      7,      4,     82,    199, 138488,\n",
      "             82,  52193, 100231,  10840,    253, 180150,   5460,  90364,    253,\n",
      "             96,     25,    446,  42077,      8,     21,  29806,  71357,     13,\n",
      "              5,    821,     25,  28792,   3075,  14919,     96,     25,    670,\n",
      "           6371,    253,  76662,    199,  32565,     82,    199,  43079,     41,\n",
      "           1647,     25,    508,  84494,     90,    253,    224,   7675,      8,\n",
      "          82641,     78,    291,  89261,    437,  73487,  13388,      5,    845,\n",
      "         232182,     22,   1810,    107,  12036,   3480,     21, 155358,      8,\n",
      "          39452,   1363,   1609,     96,     25,    670,   6371,     82,      4,\n",
      "             78,    291,  89261,    437,  73487,  13388,      4,     55,    653,\n",
      "             25,    508,  68429, 129344,    253, 101703,  55230,    253,    775,\n",
      "          59926,      5,    845,  52268,      7,  14919,     41,     96,     25,\n",
      "            670,   6371,   4372,   3900,  32395,     56,    224,  34919,      4,\n",
      "            113,  48402,    224,  34919, 176288,      4,    253,     96,     25,\n",
      "            309,      8,   3401, 138488,      4,  10274,   2970,   7675,      8,\n",
      "             96,     25, 169703,      8,     21,  89261,      4,  10274,      4,\n",
      "             78,  31929,      9,    318,    437,  73487,  13388,      4,   2970,\n",
      "           7675, 231433,   1029,   8493,    807,     21, 155358,      8,  39452,\n",
      "           1363,    115,      6,  89072,      5,    845,  52268,      7,     22,\n",
      "           1810,    107,   1103,     25,    379,    653,     25,     53,     10,\n",
      "          50734, 188118, 171197,   7722,  13993,     21,  71746,    115, 123054,\n",
      "            104,     25,  81317,      7,      4,    113,  48402,     22,   4611,\n",
      "              8,    351,      9,      7, 188786,    628,      8,    456,   4249,\n",
      "              4,    628,     78,    773,  39452,   1363,    437,  61047,   4170,\n",
      "            732,     41,     95,  56578,   1236,    108, 137464,    452,     95,\n",
      "           1774,      8,  13833, 209760,    807,    199,  93040,      7,  82791,\n",
      "              7,    628,     41,  23814,      9,   1059,    437, 164276,   3739,\n",
      "              8,  21164,  61379,  24500,      7,   2733,  34418,     21,  49907,\n",
      "         178805,   2320,      8,     96,     25,    670,   6371,      5,    339,\n",
      "             25,    670,   6371,     10,     95,  29582,      8,      6,  34590,\n",
      "            720,    199,   5595,  54916,    628,    104,     25,  58218,    603,\n",
      "           5736, 154686,    104,     25,  19988, 127989,    628,  26359,      5,\n",
      "          59235,    152,   3775,    152,    438,     39,    152,    786,  37214,\n",
      "          99428,    152,  24479,   5547,    628,    531,   9901,      8,  31124,\n",
      "          26455,  68819,    169,   1747, 100587,     95,   9023,   8918,     64,\n",
      "         176834,      8,     21,  33590,    569,  45618,     21,  19676,    531,\n",
      "           9901,    104,     25,   1811,  49894,     16,   5906,    446,  25512,\n",
      "             82,  23414,    152,  26729, 141762,    152,  72791,   2149,      8,\n",
      "         164492,    201,  24479,  77035,  77035,    427,  44586,  32196,    366,\n",
      "            164,      0,      0,      0,      0,      0,      0,      0]]), 'bbox': tensor([[[165, 112, 189, 127],\n",
      "         [193, 112, 258, 128],\n",
      "         [193, 112, 258, 128],\n",
      "         ...,\n",
      "         [  0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0],\n",
      "         [  0,   0,   0,   0]]]), 'attention_mask': tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.]]), 'entities': [{'start': [33, 57, 63, 78, 95, 441, 443, 445, 448, 450, 452, 453, 483, 485, 489, 497, 498, 499, 500], 'end': [57, 63, 78, 95, 99, 443, 445, 448, 450, 452, 453, 483, 485, 489, 492, 498, 499, 500, 505], 'label': [1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2]}], 'relations': [{'start_index': [], 'end_index': [], 'head': [], 'tail': []}]}\n",
      "input_ids shape: torch.Size([1, 512])\n",
      "bbox shaep: torch.Size([1, 512, 4])\n",
      "attention_mask shape: torch.Size([1, 512])\n",
      "output: RelationExtractionOutput(loss=tensor(1.6006), pred_relations=[[{'head_id': 6, 'head': (443, 445), 'head_type': 1, 'tail_id': 17, 'tail': (499, 500), 'tail_type': 2, 'type': 1}, {'head_id': 14, 'head': (489, 492), 'head_type': 1, 'tail_id': 15, 'tail': (497, 498), 'tail_type': 2, 'type': 1}, {'head_id': 9, 'head': (450, 452), 'head_type': 1, 'tail_id': 10, 'tail': (452, 453), 'tail_type': 2, 'type': 1}, {'head_id': 0, 'head': (33, 57), 'head_type': 1, 'tail_id': 4, 'tail': (95, 99), 'tail_type': 2, 'type': 1}, {'head_id': 0, 'head': (33, 57), 'head_type': 1, 'tail_id': 2, 'tail': (63, 78), 'tail_type': 2, 'type': 1}, {'head_id': 5, 'head': (441, 443), 'head_type': 1, 'tail_id': 16, 'tail': (498, 499), 'tail_type': 2, 'type': 1}, {'head_id': 0, 'head': (33, 57), 'head_type': 1, 'tail_id': 1, 'tail': (57, 63), 'tail_type': 2, 'type': 1}, {'head_id': 0, 'head': (33, 57), 'head_type': 1, 'tail_id': 3, 'tail': (78, 95), 'tail_type': 2, 'type': 1}, {'head_id': 5, 'head': (441, 443), 'head_type': 1, 'tail_id': 17, 'tail': (499, 500), 'tail_type': 2, 'type': 1}, {'head_id': 11, 'head': (453, 483), 'head_type': 1, 'tail_id': 12, 'tail': (483, 485), 'tail_type': 2, 'type': 1}, {'head_id': 13, 'head': (485, 489), 'head_type': 1, 'tail_id': 18, 'tail': (500, 505), 'tail_type': 2, 'type': 1}, {'head_id': 7, 'head': (445, 448), 'head_type': 1, 'tail_id': 8, 'tail': (448, 450), 'tail_type': 2, 'type': 1}]], entities=[{'start': [33, 57, 63, 78, 95, 441, 443, 445, 448, 450, 452, 453, 483, 485, 489, 497, 498, 499, 500], 'end': [57, 63, 78, 95, 99, 443, 445, 448, 450, 452, 453, 483, 485, 489, 492, 498, 499, 500, 505], 'label': [1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2, 2]}], relations=[{'start_index': [], 'end_index': [], 'head': [], 'tail': []}], hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "XFUND_JSON = \"/mnt/j/dataset/document-intelligence/XFUND/fr\"\n",
    "image_path = \"/mnt/j/dataset/document-intelligence/XFUND/fr/fr.val\"\n",
    "\n",
    "from utils.image_utils import *\n",
    "\n",
    "label2id = {\"HEADER\":0, \"QUESTION\":1, \"ANSWER\":2}\n",
    "\n",
    "with open(os.path.join(XFUND_JSON, \"fr.val.json\"), encoding=\"utf-8\", mode=\"r\") as fi:\n",
    "    ann_infos = json.load(fi)\n",
    "    document_list = ann_infos[\"documents\"]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(MP, \"xlm-roberta-base\"))\n",
    "\n",
    "for guid, doc, in enumerate(document_list):\n",
    "    image_file = os.path.join(image_path, doc[\"img\"][\"fname\"])\n",
    "    print(image_file)\n",
    "    size = [doc[\"img\"][\"width\"], doc[\"img\"][\"height\"]]\n",
    "    image, size = load_image(image_file, size=224)\n",
    "    original_image, _ = load_image(image_file)\n",
    "    document = doc[\"document\"]\n",
    "    tokenized_doc = {\"input_ids\": [], \"bbox\": [], \"labels\": []}\n",
    "    entities = []\n",
    "    relations = []\n",
    "    id2label = {}\n",
    "    entity_id_to_index_map = {}\n",
    "    empty_entity = set()\n",
    "    for line in document:\n",
    "        if len(line[\"text\"]) == 0:\n",
    "            empty_entity.add(line[\"id\"])\n",
    "            continue\n",
    "        id2label[line[\"id\"]] = line[\"label\"]\n",
    "        relations.extend([tuple(sorted(l)) for l in line[\"linking\"]])\n",
    "        # 将文本转为id\n",
    "        # offset_mapping：每一个token具体占了几个char，中文都是一个char，英文就看单词的长度， [PAD]全都是0\n",
    "        tokenized_inputs = tokenizer(\n",
    "            line[\"text\"],\n",
    "            add_special_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "        )\n",
    "        text_length = 0\n",
    "        ocr_length = 0\n",
    "        bbox = []\n",
    "        for token_id, offset in zip(tokenized_inputs[\"input_ids\"], tokenized_inputs[\"offset_mapping\"]):\n",
    "            if token_id == 6:\n",
    "                bbox.append(None)\n",
    "                continue\n",
    "            text_length += offset[1] - offset[0]\n",
    "            tmp_box = []\n",
    "            while ocr_length < text_length:\n",
    "                ocr_word = line[\"words\"].pop(0)\n",
    "                ocr_length += len(\n",
    "                    tokenizer._tokenizer.normalizer.normalize_str(ocr_word[\"text\"].strip())\n",
    "                )\n",
    "                tmp_box.append(simplify_bbox(ocr_word[\"box\"]))\n",
    "            if len(tmp_box) == 0:\n",
    "                tmp_box = last_box\n",
    "            bbox.append(normalize_bbox(merge_bbox(tmp_box), size))\n",
    "            last_box = tmp_box  # noqa\n",
    "        #\n",
    "        bbox = [\n",
    "            [bbox[i + 1][0], bbox[i + 1][1], bbox[i + 1][0], bbox[i + 1][1]] if b is None else b\n",
    "            for i, b in enumerate(bbox)\n",
    "        ]\n",
    "        # 生成每个字或者词的标签\n",
    "        if line[\"label\"] == \"other\":\n",
    "            label = [\"O\"] * len(bbox)\n",
    "        else:\n",
    "            label = [f\"I-{line['label'].upper()}\"] * len(bbox)\n",
    "            label[0] = f\"B-{line['label'].upper()}\"\n",
    "        tokenized_inputs.update({\"bbox\": bbox, \"labels\": label})\n",
    "        if label[0] != \"O\":\n",
    "            entity_id_to_index_map[line[\"id\"]] = len(entities)\n",
    "            entities.append(\n",
    "                {\n",
    "                    \"start\": len(tokenized_doc[\"input_ids\"]),\n",
    "                    \"end\": len(tokenized_doc[\"input_ids\"]) + len(tokenized_inputs[\"input_ids\"]),\n",
    "                    \"label\": line[\"label\"].upper(),\n",
    "                }\n",
    "            )\n",
    "        for i in tokenized_doc:\n",
    "            tokenized_doc[i] = tokenized_doc[i] + tokenized_inputs[i]\n",
    "    relations = list(set(relations))\n",
    "    relations = [rel for rel in relations if rel[0] not in empty_entity and rel[1] not in empty_entity]\n",
    "    kvrelations = []\n",
    "    for rel in relations:\n",
    "        pair = [id2label[rel[0]], id2label[rel[1]]]\n",
    "        if pair == [\"question\", \"answer\"]:\n",
    "            kvrelations.append(\n",
    "                {\"head\": entity_id_to_index_map[rel[0]], \"tail\": entity_id_to_index_map[rel[1]]}\n",
    "            )\n",
    "        elif pair == [\"answer\", \"question\"]:\n",
    "            kvrelations.append(\n",
    "                {\"head\": entity_id_to_index_map[rel[1]], \"tail\": entity_id_to_index_map[rel[0]]}\n",
    "            )\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    def get_relation_span(rel):\n",
    "        bound = []\n",
    "        for entity_index in [rel[\"head\"], rel[\"tail\"]]:\n",
    "            bound.append(entities[entity_index][\"start\"])\n",
    "            bound.append(entities[entity_index][\"end\"])\n",
    "        return min(bound), max(bound)\n",
    "\n",
    "    relations = sorted(\n",
    "        [\n",
    "            {\n",
    "                \"head\": rel[\"head\"],\n",
    "                \"tail\": rel[\"tail\"],\n",
    "                \"start_index\": get_relation_span(rel)[0],\n",
    "                \"end_index\": get_relation_span(rel)[1],\n",
    "            }\n",
    "            for rel in kvrelations\n",
    "        ],\n",
    "        key=lambda x: x[\"head\"],\n",
    "    )\n",
    "    chunk_size = 512\n",
    "    for chunk_id, index in enumerate(range(0, len(tokenized_doc[\"input_ids\"]), chunk_size)):\n",
    "        item = {}\n",
    "        for k in tokenized_doc:\n",
    "            item[k] = tokenized_doc[k][index: index + chunk_size]\n",
    "        entities_in_this_span = []  # 把相同的实体合并在一起?\n",
    "        global_to_local_map = {}\n",
    "        for entity_id, entity in enumerate(entities):\n",
    "            if (\n",
    "                    index <= entity[\"start\"] < index + chunk_size\n",
    "                    and index <= entity[\"end\"] < index + chunk_size\n",
    "            ):\n",
    "                entity[\"start\"] = entity[\"start\"] - index\n",
    "                entity[\"end\"] = entity[\"end\"] - index\n",
    "                global_to_local_map[entity_id] = len(entities_in_this_span)\n",
    "                entities_in_this_span.append(entity)\n",
    "        relations_in_this_span = []\n",
    "        for relation in relations:\n",
    "            if (\n",
    "                    index <= relation[\"start_index\"] < index + chunk_size\n",
    "                    and index <= relation[\"end_index\"] < index + chunk_size\n",
    "            ):\n",
    "                relations_in_this_span.append(\n",
    "                    {\n",
    "                        \"head\": global_to_local_map[relation[\"head\"]],\n",
    "                        \"tail\": global_to_local_map[relation[\"tail\"]],\n",
    "                        \"start_index\": relation[\"start_index\"] - index,\n",
    "                        \"end_index\": relation[\"end_index\"] - index,\n",
    "                    }\n",
    "                )\n",
    "        item.update(\n",
    "            {\n",
    "                \"id\": f\"{doc['id']}_{chunk_id}\",\n",
    "                \"image\": image,\n",
    "                \"original_image\": original_image,\n",
    "                \"entities\": entities_in_this_span,\n",
    "                \"relations\": relations_in_this_span,\n",
    "            }\n",
    "        )\n",
    "        print(\"item:\", item)\n",
    "        model_input = {}\n",
    "        input_ids = [0]*512\n",
    "        input_ids[0:len(item[k])] = item[\"input_ids\"]\n",
    "        model_input[\"input_ids\"] = torch.LongTensor([input_ids])\n",
    "        \n",
    "        bbox = []\n",
    "        for i in range(512):\n",
    "            bbox.append([0,0,0,0])\n",
    "        print(\"ccc:\", bbox)\n",
    "        bbox[0:len(item[k])] = item[\"bbox\"]\n",
    "        model_input[\"bbox\"] = torch.LongTensor([bbox])\n",
    "        \n",
    "        attention_mask = [0]*512\n",
    "        attention_mask[0:len(item[k])] = [1]*len(item[k])\n",
    "        model_input[\"attention_mask\"] = torch.FloatTensor([attention_mask])\n",
    "        \n",
    "        print(\"label2id:\", label2id)\n",
    "        entity_dict = {'start': [entity[\"start\"] for entity in item[\"entities\"]],\n",
    "                        'end': [entity[\"end\"] for entity in item[\"entities\"]],\n",
    "                        'label': [label2id[entity[\"label\"]] for entity in item[\"entities\"]]}\n",
    "        print(\"entity_dict:\", entity_dict)\n",
    "\n",
    "        model_input[\"entities\"] = [entity_dict]\n",
    "        model_input[\"relations\"] = [{'start_index': [], 'end_index': [], 'head': [], 'tail': []}]\n",
    "        \n",
    "        print(\"model_input:\", model_input)\n",
    "        print(\"input_ids shape:\", model_input[\"input_ids\"].shape)\n",
    "        print(\"bbox shaep:\", model_input[\"bbox\"].shape)\n",
    "        print(\"attention_mask shape:\", model_input[\"attention_mask\"].shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # inputs: ocr处理侯的输入，ids（SER和RE的inputs是一的）\n",
    "            outputs = model(**model_input)\n",
    "            print(\"output:\", outputs)\n",
    "        \n",
    "       \n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf247f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda9e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
